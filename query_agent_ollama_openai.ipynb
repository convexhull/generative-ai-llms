{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os \n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from typing import List\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "openAI = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_prompt = \"You are an agent that specializes in answering questions related to software development, code, LLMs, AI and programming. Give precise answers to user's questions. Respond in clearly formatted markdown\"\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\"\n",
    "\n",
    "def get_user_prompt(question):\n",
    "    user_prompt = f\"\"\"\n",
    "    Answer the following question\n",
    "    {question}\n",
    "    \"\"\"\n",
    "    return user_prompt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get answer using gpt-4o-mini, with streaming\n",
    "\n",
    "def answer_gpt_with_streaming(question):\n",
    "    stream = openAI.chat.completions.create(model=MODEL_GPT, messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\" : get_user_prompt(question)},\n",
    "    ],\n",
    "    stream=True)\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "            response += chunk.choices[0].delta.content or ''\n",
    "            response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "            update_display(Markdown(response), display_id=display_handle.display_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05700c10-e17e-42a6-aa98-9bb0677074f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_gpt_with_streaming(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get answer using Ollama running llama3.2\n",
    "\n",
    "def answer_ollama(question):\n",
    "    LLAMA_ENDPOINT_URL = \"http://localhost:11434/api/chat\"\n",
    "    HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"model\": MODEL_LLAMA,\n",
    "        \"messages\": [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": get_user_prompt(question)} ],\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    response = requests.post(LLAMA_ENDPOINT_URL, json=payload, headers=HEADERS)\n",
    "    display(Markdown(response.json()['message']['content'])) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286a70e6-951a-454a-b41e-47c030542fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_ollama(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
